---
layout: post
title: Machine Learning - Week 1
tags: [Machine Learning, Coursera]
---

**Screenshots and some notes** are credited to machine learning@Stanford, Coursera
Well, I mainly use this blog to take notes and review some materials.

## Introduction
- What is Machine Learning?
```
	Arthur Samuel(1959): filed of study that gives computers the ability to learn without being explicitly programmed. => Older Definition

	Tom Mitchell(1988): A computer program is said to learn from experience E with respect to some task T and some performance P, if its performance on T, as measured by P, improves with experience E.
```
<!--excerpt-->

	Here is the example that helps to understand the definition above.
    <picture>
    <img src = "/blog/image/ml_intro_example.png" width="80%" height="80%">
    </picture>

- What is Supervised Learning?
```
	Basically, supervised learning is saying that we are given a dataset and already known what our correct outpurs. What we want to do is to find a relationship between inputs and outpus. Later, we can use this relationship to answer some related questions

    Well, it is like we are given X(input) and Y(output), and we want to find a function F such that F(X) = Y.
```

	**Supervised learning** problems are categorized into "**regression**" and "**classification**" problems.

    In the regression problem, we are trying to map input variables to some continuous function.

    In the classification problem, we are trying to map input variables into discrete categories.

	Here is the example that helps to understand the definition above.
    <picture>
	<img src = "/blog/image/sl_example.png" width = "80%" height = "80%">
    </picture>

- What is Unsupervised Learning?
```
	Basically, supervised learning is saying that the dataset does not have any labels and we want to find a structure in the data. We can derive this structure by clustering the data based on relationships among the variables in the data.
```

	For unsupervised learning, what we usually do is to segment the given dataset and find clusters, or find a certain structure from the dataset.

## Linear Regression with One Variable

Linear regression is one set of supervised learning. You are given a dataset with some features and their corresponding outputs. And you want to find a relationship between these features and their outputs. Now, the first thing is to know how to use model(function) to represent this relationship.

### Model Representation
(X,Y): the training set
(x^i^, y^i^): the ith traning data

<picture>
<img src = "/blog/image/model_representation.png" width = "80%" height = "80%">
</picture>


So, h here can be regarded as the mappfing function that fits the traning dataset (X,Y).

### Cost Function
Since there exists many good mapping functions, the cost function helps us to figure out the best strategy to find our function h such that it fits the training dataset well.

Suppose that we have a hypothesis: h(x) = a + bx, where a and b are called **parameters** and there are many options for a and b, and how do we choose the best a, b to fit the data?

<picture>
<img src = "/blog/image/cf_example.png" width = "80%" height = "80%">
</picture>

So, ***J*** here is the cost function, also called "Squared Error Function" and "Mean Error Function". The objective of it is to find a theta^1^ and theta^0^ such that the cost function is minimized. Notice that the coefficient `1/2` here is for computational convenience since the derivative of the cost function will cancel out the `1/2` term.

Well, from mathematics' view, I think we can think of this problem as finding the global minimum of this function. There are many ways to do that, such as taking derivatives and verifying the value at the extreme points. From CS' view, gradient descent is one algorithm that optimizes the objective function.

### Gradient Descent
To well understand this algorithm, we can see an example used in the lession.

<picture>
<img src = "/blog/image/gdm_example1.png" width = "50%" height = "50%"><img src = "/blog/image/gdm_example2.png" width = "50%" height = "50%">
</picture>

These two figures are actually displaying the cost functions against two parameters. But now we can image that the cost function is actually a mountain and you are standing somewhere on the mountain. What you want to do is looking for a path that can lead you to the lowest point on this mountain, which is actually the minimum value of the cost function. Any idea?

Well, it turns out that the best strategy is to follow the steepest slope at the point where you are standing. We can achieve this by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at the point and it will give us a direction to move forwards.

Now, for each step, we want to determine how much this step will be by multiplying the derivative by a parameter, called the learning rate. It then gives us the following equation.

<picture>
<img src = "/blog/image/gda.png" width = "80%" height = "80%">
</picture>

The equation then is straightforward. But we need to pay attention to the update rule. Notice that every parameter must be updated simultaneously, otherwise, as shown in the figure above, the updated parameters will be used to update other non-updated parameters.

Now, here comes a question when I was learning this section: **what if the algorithm fails to choose the best strategy to update paramters?** There are many directions to update parameters, and the cost function may **get stuck at the local minimum** instead of reaching the global minimum. (1)

Another question is that **how to choose an appropriate learning rate**? What will happen **if the learning rate is too big** or **too small**? (2)

Also, since the learning rate cannot be changed, and now the cost function is extremely close to the global minimum, and after updating parameters, the cost function exceeds the global minimum and it needs go back. **What if it keeps swinging around the global minimum?** How to avoid this? (3)








