---
layout: post
title: Machine Learning - Week 2
tags: [Machine Learning, Coursera]
---

**Screenshots and some notes** are credited to machine learning@Stanford, Coursera.

## Multiple Features

In the week 1, it mentions about using one feature, i.e. the size of the room to determine the price of a house. The multiple features (variables) here talks about using multiple features, such as size, number of bedrooms, number of floors etcs, to determine the price. If we take it in a mathematical way, we can write it as **$F(x_1, x_2, x_3, ..., x_n) = y$**.

<!--excerpt-->

``Notation``:

n = number of features

$x^i$ = input features of $i^{th}$ training example

$x_j^i$ = value of feature j in $i^{th}$ training example

The hypothesis turns out to be **$h_{\theta}(x) = {\theta}_0+{\theta}_1x_1+{\theta}_2x_2+{\theta}_3x_3+...+{\theta}_nx_n$**. We can also write this in matrix form by considering $x_0=1$. Then, $x = (x_0, x_1,, ..., x_n) \in R^{n+1}$ and $\theta = (\theta_0, \theta_1,, ..., \theta_n) \in R^{n+1}$.

Then, it follows that **$h_{\theta}(x) = \theta x^{T}$**, where $x^T$ is the transpose of the matrix $x$, and now, we can consider this hypothesis as an inner product of the features (varaibles) $x$ and their corresponding weights (parameters) $\theta$ and this hypothesis is also called ***MULTIVARIATE LINEAR REGRESSION***.

## Gradient Descent for Multiple Variables
