---
layout: post
title: Machine Learning - Week 2
tags: [Machine Learning, Coursera]
---

**Screenshots and some notes** are credited to machine learning@Stanford, Coursera.

## Multiple Features

In the week 1, it mentions about using one feature, i.e. the size of the room to determine the price of a house. The multiple features (variables) here talks about using multiple features, such as size, number of bedrooms, number of floors etcs, to determine the price. If I take it in a mathematical way, I can write it as **$F(x_1, x_2, x_3, ..., x_n) = y$**.

<!--excerpt-->

``Notation``:

n = number of features

$x^i$ = input features of $i^{th}$ training example

$x_j^i$ = value of feature j in $i^{th}$ training example

The hypothesis turns out to be **$h_{\theta}(x) = {\theta}_0+{\theta}_1x_1+{\theta}_2x_2+{\theta}_3x_3+...+{\theta}_nx_n$**. I can also write this in matrix form by considering $x_0=1$. Then, $x = (x_0, x_1,, ..., x_n) \in R^{n+1}$ and $\theta = (\theta_0, \theta_1,, ..., \theta_n) \in R^{n+1}$.

Then, it follows that **$h_{\theta}(x) = \theta x^{T}$**, where $x^T$ is the transpose of the matrix $x$, and now, I can consider this hypothesis as an inner product of the features (varaibles) $x$ and their corresponding weights (parameters) $\theta$ and this hypothesis is also called ***MULTIVARIATE LINEAR REGRESSION***.

## Gradient Descent for Multiple Variables

Now, I have **$h_{\theta}(x) = \theta x^{T}$ = ${\theta}^{T}x$**, the multivariate linear regression function. The end-goal here is to fit the hypothesis in the dataset so that I can predict the value when given some features. So, the next step is to look for a path to get there, which reminds me of the gradient descent that I learned in the previous week.

What the gradient descent does is to **update the parameters $\theta$** so that the hypothesis function is **close** to the "ground-truth" function, or the value evaluated through the hypothesis function is "almost the same" as the value computed by the "ground-truth" function. (*"ground-truth" is a term usually used in computer vision*.)

So, here comes the cost function: **$J(\theta)={\frac{1}{2m}}\sum_{i=1}^{m}(h_{\theta}(x^{i}) - y^{i})^{2}$**. If the value returned by the cost function is small enough, then the hypothesis function is good enough to do the prediction. Since the feature values will not change, we could use gradient descent to update the $\theta$ (weights).

For one varialbe, it is $\theta = \theta - \alpha\frac{\partial}{\partial \theta} J(\theta)$, where {\alpha} is the learning rate. For multiple variables, it is $\theta_{j} = \theta_{j} - \alpha\frac{\partial}{\partial \theta_{j}} J(\theta)$, where we simultaneously update for every $j = 0, ..., n$.

## Feature Scaling

The idea here is to make sure features are on a similar scale.

Why? Look at this example.

<picture>
<img src = "/blog/image/feature_scaling1.png" width = "80%" height = "80%">
</picture>

The figure represents the contour of the cost function $J(\theta)$ where $\theta = (\theta_0, \theta_1, \theta2)$ and $\theta_0$ is ignored. Now, $x_1$ and $x_2$ have different feature scales and since $x_1$ has larger range of values than $x_2$ does, it turns out that the contour is a very tall, skinny and skewed elliptical shape. If we run gradient descent on this cost-function, the gradients may end up taking a long time and can oscillate back and forth before it can find its way to the global minimum.

Therefore, we should take feature scaling into consideration, which can normalize the values into an appropriate range such that gradients can converge much more quickly. (Well, this concept reminds me of **normalization**.)

<picture>
<img src = "/blog/image/feature_scaling2.png" width = "80%" height = "80%">
</picture>

Definition of feature scaling: Get every feature into approximately a $-1 \leq x_i \leq 1$ range. Actually, from the lecture, this scaling is not very strict. We can also let the range be $[-3, 3]$ as long as the boundary values are not too large or too small.

> Question: Actually, at this point, I know it is necessary to do feature scaling (normalization) but I don't know why it can help gradient descent converge faster. And when should I use the normalization? It seems that it is safe to use it every time when we meet multiple features. Moreover, why does it improve the accuracy?
>
> Stackoverflow, Google, etc save me:
>
> ***For question 1***, it helps gradient descent converge faster becauses it lowers the standard derivation of feature values. $\theta$ descend quickly on small ranges and slowly on large ranges, then it will oscillate inefficiently down to the optimum when the variables are very uneven.
>
> ***For question 2***, in general, scaling ensures that some features are not too big or too small and prevents them being used as the main predictor. Therefore, we should normalize when the scale of feature is irrelevant or misleading and not normalize when the scale is meaningful. And this is related to the prior knowledge you have about the problem.
> Some of the algorithms, like Naive Bayes, do feature scaling by design and there will be no effect by manually adding scaling. Others, like k-nearest neighbors (knn) are gravely affected by it since the distance between feature points matters.
>
> ***For question 3***, the common practice in unsupervised machine learning algorithms about hyper-parameters selection is that you could have any personal subjective assumption about the data, therefore, it is good that different features share equal possibility to appear. The feature scaling just try to make the assumption that all the features have equal oppotunities to influence the weight.

### Mean normalization

Definition: replace $x_i$ with $x_i - \mu_i$ to make features have approximately zero mean. Mathematical formula: $x_i = \frac{x_i - \mu_i}{\sigma^2}$.

> Question: Why mean normalization is helpful?
>
> ***For this question***, essentially, scaling the inputs gives the error surface a more spherical shape, otherwise it would generate a surface with high curvature. Since the gradient descent is curvature-ignorant, having a high curvature surface means that it will take many unnecessarily steps before reaching the optimum. Therefore, when we perform scaling on the features, we are trying to lower thr curvature, making it faster for gradient descent.